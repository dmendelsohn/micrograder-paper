\documentclass[12pt]{article}
\usepackage{textcomp}
\usepackage[margin=1.0in]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{setspace}
\usepackage{indentfirst}

\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\lstMakeShortInline[columns=fixed]|


\newcommand{\mytitle}{\textbf{An Automatic Grader for Embedded Systems Courses}}
\newcommand{\mydate}{August 18, 2017}

\begin{document}

\begin{titlepage}

\centering
\mytitle \\
\vspace{12pt}
by Daniel Mendelsohn \\
MIT S.B., 2015 \\
\vspace{12pt}
Submitted to the \\
Department of Electrical Engineering and Computer Science \\
in Partial Fulfillment of the Requirements for the Degree of \\
\vspace{12pt}
Master of Engineering in Electrical Engineering and Computer Science \\
\vspace{12pt}
at the \\
\vspace{12pt}
Massachusetts Institute of Technology \\
\vspace{12pt}
August 2017 \\
\vspace{12pt}
\textcopyright \hspace{0.05in} Massachusetts Institute of Technology 2017.  All rights reserved. \\
\vspace{48pt}

Author \dotfill \\
\begin{flushright}
Department of Electrical Engineering and Computer Science \\
\mydate
\end{flushright}
\vspace{36pt}

Certified by \dotfill \\
\begin{flushright}
Joseph Steinmeyer \\
Thesis Supervisor \\
\mydate
\end{flushright}
\vspace{24pt}

Accepted by \dotfill \\
\begin{flushright}
Dr. Christopher J. Terman \\
Chairman, Masters of Engineering Thesis Committee
\end{flushright}

\end{titlepage}

\addtocounter{page}{1}

\newpage
\mbox{}
\newpage

\begin{center}
\mytitle \\
by \\
Daniel Mendelsohn \\
\vspace{12pt}
Submitted to the Department of Electrical Engineering and Computer Science\\
 on \mydate{}, in Partial Fulfillment of the Requirements for the Degree of\\
 Master of Engineering in Electrical Engineering and Computer Science
\end{center}
\vspace{12pt}
\textbf{ABSTRACT} \\

\noindent TODO: write this

\newpage
\mbox{}
\newpage

\tableofcontents

\doublespacing

\newpage
\section{Introduction}
Write about 6.S08, the current situation, how its a pain, etc

Write about how ``regular" software checkers are a solved problem

Write about unique challenges of embedded systems (computing platform challenges, physical vs digital abstractions, heavily time-dependent in a way typical software classes aren't)

\newpage
\section{Prior Work}
Write about UT Austin's EdX class, and the system they use

Discuss strengths and weaknesses of the UT Austin approach

\newpage
\section{Design Principles}
Before building MicroGrader, I outlined a set of design principles to serve as a foundation for my technical design.

\subsection{Platform agnosticism}
The microcontroller ecosystem is fractured and changes quickly.  Unsurprisingly, a wide variety of platforms are currently used in embedded systems education.  For example, MIT's 6.S08 uses the Teensy (a third-party Arduino variant) while UT Austin's Embedded Systems course on EdX uses the TM4C123 (an ARM-based board made by Texas Instruments).  Ideally, the automated grader should not care if a project is implemented on a Teensy or a TM4C123, as long as it meets the appropriate specification. The architecture of the grader must not be intrinsically tied to any one microcontroller or any specific sensors.  The interfaces ought to be as generic as possible.

\subsection{Extensibility}
It should be possible to define new data types for assessment.  In 6.S08, students learn to work with a fairly standard set of sensors and other peripherals.  The main system inputs are digital GPIO, analog GPIO, and a nine-axis IMU.  The automatic grader should \textbf{not} be restricted to these types of inputs.  For example, if a course assignment involves interfacing with a temperature sensor, the grader ought to be handle that.

This extensibility could even make it possible to use the grader in domains other than EECS.  In principle, an embedded system provides an interface between the physical world and the digital world.  MicroGrader could therefore be used to grade a project with any kind of electronically measurable output.

\subsection{Actionable feedback for students}
Students should be able to get explanations of each passed and failed test case.  This may require significant technical work -- the internal representation of a test case might be complicated, but the explanations should be simple.  Based on the description of a failed test, a student should know exactly what output the system observed and what output the system expected to observe.

\subsection{Ease of use for students}
In terms of the student experience, MicroGrader should be seamless to use.  Running tests should be as simple as setting a flag at the top of the program.  Any embedded libraries that the students use should be configured to use that flag to determine whether to run normally or to run in ``test" mode.  Students should not have to reorganize or rewrite a functioning project in order to coax the grader into working properly.

\subsection{Ease of use for course staff}
My experience with tedious manual grading in 6.S08 inspired this project, in large part.  If building test cases in MicroGrader is actually more time-intensive than manual grading, this project will have failed its primary purpose.  The time required to customize the grader for a specific assignment should be roughly proportional to the complexity of the assignment.  If possible, a working implementation of the assignment should be sufficient to programmatically generate a test.\\

MicroGrader should impose as few constraints as possible on the assignments it grades.  In a perfect world, it would be able to evaluate if an embedded system implements any conceivable specification.

\subsection{Minimize false positives}
Although few students would complain about invalid solutions occasionally passing automated tests, it inevitably does them a disservice.  In 6.S08, for example, the students are mostly newcomers to programming and have not yet developed sophisticated debugging skills or proper testing disciplines.  Students have difficulty shedding the assumption that code is correct just because it passed the tests.  If that code is critical in a future exercise, it will cause problems.

\subsection{Minimize false negatives}
It's discouraging and perplexing for students when apparently correct solutions fail automated tests.  Ideally, any functional solution -- not just the intended one -- should pass the tests.  It's a challenge to avoid ``Heisenbugs", wherein the system works under normal operation, but fails during testing due to changes introduced by the test itself.  In practice, I've found it necessary to develop a set of guidelines to avoid such situations (e.g. maximum recommended frequency for certain I/O events).

\newpage
\section{Defining a test case}
We can think of a specification as a mapping from a set of input signals to a set of output signals.  In this system, signals are assumed to be piecewise constant.  A piecewise constant signal can approximate any piecewise continuous signal by increasing the resolution.  Not all possible specifications can be evaluated by MicroGrader.  For the sake of simplicity, the internal structures of MicroGrader only support a subset of all possible specifications, though this subset is quite broad.

\subsection{Channels}
We label each individual input and output as a \textit{channel}.  Specifically, a channel is comprised of a data type and a sub-channel.  For example, ``digital input" is a data type, the pin number is the sub-channel.  The built-in data types, and their associated sub-channels, are:

\begin{itemize}
\item Digital Input (sub-channels are pin numbers)
\item Digital Output (sub-channels are pin numbers)
\item Analog Input (sub-channels are pin numbers)
\item Analog Outputs (sub-channels are pin numbers)
\item Accelerometer Input (sub-channels are axes)
\item Gyroscope Input (sub-channels are axes)
\item Magnetometer Input (sub-channels are axes)
\item Monochrome Display Output (no sub-channels)
\end{itemize}

\subsection{Static inputs}
\label{sec:static-inputs}
In a test case, the inputs signals are more-or-less statically pre-defined.  This is in contrast to other automatic graders that deputize the student to perform actions that generate the inputs.  The details of how these pre-defined inputs are delivered to the student's program will be described later.  As mentioned above, input signals are modeled as a piecewise constant function in time, with arbitrarily high time resolution.

Allowing for dynamic input generation at evaluation-time would greatly complicate the implementation and design of the system, and is outside the scope of this project.  Typically, this option is chosen out of necessity, as other automated graders lack a way to deliver pre-define inputs.  Dynamically generating inputs with human action is actually quite restrictive.

\subsection{Evaluation of outputs}
Outputs are evaluated ``offline" in the algorithmic sense.  That is, the outputs of the embedded system are collected in full, and then evaluated once the test is finished (for some definition of ``finished").  Furthermore, for the sake of simplicity, MicroGrader considers each output channel independently.  As implemented, there is no cross-modal evaluation of separate output channels.

Collected outputs are samples of continuous-time signal.  We use the simplest method of mapping the discrete-time sampling to a continuous-time signal; at any given time, we consider the most recent sample of the output to be the current value of that output.  This turns out to be pretty reasonable in the context of many embedded system outputs.  On most microcontrollers, output values such as an electrical voltage are ``held" until a new output value is specified.

\subsubsection{Data structure: Evaluation Point}
\label{sec:eval-point}
Our fundamental unit of output evaluation is the \textit{evaluation point}.  An evaluation point consists of:

\begin{itemize}
\item Output Channel (e.g. ``analog output on pin 5")
\item Time Interval (a numeric start time and end time)
\item Expected Value (the ``correct" value of the given output channel in this interval)
\item Check Function (a boolean function of two arguments)
\item Required Portion (a float between 0 and 1, inclusive)
\item Condition (we'll discuss this later)
\end{itemize}

The check function takes an expected value and an observed value as input, and returns a boolean.  We consider that boolean to be the correctness of the observed value with respect to the expected value.

In order to evaluate an evaluation point, we consider the reconstructed continuous signal for the point's channel, during the point's time interval.  We then calculate the portion of the interval for which the observed value of the signal is correct, with respect to the point's expected value.  If that portion is greater than or equal to the point's ``required portion", then the point evaluates to |true|.  Otherwise, it evaluates to |false|.

The point's time interval isn't an absolute time interval, but rather a relative time interval, where $t=0$ is the time at which the point's ``condition" is met.  We'll discuss this is more detail in section \ref{sec:condition}.

\subsubsection{Aggregating point results}
For any channel, we calculate an overall test result in a flexible way.  Specifically, an ``aggregator function" is defined for each channel.  This function takes the set of boolean point results for that channel as input, and returns a decimal score in the interval $[0,1]$.  The scores for each channel are then averaged together (optionally, a weighted average can be used) to get a final score for the entire test case.

\subsection{Relative timing}
In many embedded systems projects, there are subsystems with unknown latency.  Sometimes, this latency is highly variable.  This is especially true in the context of MIT's 6.S08, due to its focus on web-connected embedded systems.  Events with highly variable latency include obtaining a WiFi connection, making an HTTP request, and obtaining a GPS fix.

In the current implementation of MicroGrader, web requests are not simulated.  Rather, they are considered to be a hidden ``internal" step between system inputs (e.g. a button is pressed) and system outputs (e.g. some text from the web is displayed).  It is not possible, at the time a test case is designed, to predict when this output ought to occur in absolute terms.  We can, however, specify when the output ought to occur in relative terms (e.g. when an HTTP response arrives).  For this reason, MicroGrader typically represents time relative to the time at which a pre-defined condition is met.

\subsubsection{Data structure: Condition}
\label{sec:condition}
In MicroGrader, there are four basic types of conditions.  Conditions and be composed together to create more complex conditions, which can represent most events we care about.

The first type of condition is the \textit{basic condition}.  The time at which a \textit{basic condition} is met is defined by a \textit{cause}.  In this case, the cause is a boolean function of that takes a single input -- a representation of an embedded system event.  The boolean return value represents whether or not the input event is considered to satisfy the condition.  For example, we can use a \textit{cause} function that returns |true| if and only if the event is a ``system initialization" event.  Another possible \textit{cause} function would return |true| if and only if the system event is a ``Wifi response" event.  Once the condition is satisfied, it remains satisfied forever.  The details of how these events are observed will be discussed later in section \ref{sec:architecture}. 

The second type of condition is the \textit{after condition}.  This type of condition has another condition (of any kind) as its ``precondition".  The \textit{cause} function of an \textit{after condition} is only invoked for system events that occur after the precondition is met.   In addition, the \textit{cause} of an \textit{after condition} can be a single number, rather than a function.  In this case, we consider this condition to be met a fixed amount of time after the precondition is met, as defined by this number.

The third type of condition is the \textit{and condition}.  This type of condition has no \textit{cause}, but rather just a set of \textit{sub-conditions}, which can be of any type.  This condition is considered to be met once \textbf{all} of the sub-conditions are met.

The fourth and final type of condition is the \textit{or condition}.  It is analogous to the \textit{and condition}, except it is considered to be satisfied once \textbf{any} of its sub-conditions are met.

\subsubsection{Data structure: Input Frames}
\label{sec:frames}
Now that we have the ability to define time in a relative way, let's re-examine how a test case defines its inputs.  Specifically, input signals are fixed relative to a condition, rather than be fixed at an absolute time.  The principal data structure for accomplishing this is the \textit{frame}.

A \textit{frame} is comprised of a \textit{start condition}, an \textit{end condition}, and a set of time-series signals (each of which is associated with a specific input channel).  The two conditions can be of any type described in section \ref{sec:condition}.  The signals must all be defined for all times $t>=0$.  The time at which the start condition is met is considered to be our ``anchor point", $t=0$.  A frame is considered \textit{active} at times at or after the start condition is met, and before the end condition is met.

A test case can (and often requires) multiple frames, since different behaviors that we'd like to evaluate occur relative to different events.  During the execution of a test, is it possible for zero frames, one frame, or multiple frames to be active at a given time.  If exactly one frame is active, that frame's values for each input channel are the test inputs to the embedded system.

If multiple frames are active at a time $t$, we must decide which frame is used to determine the system inputs.  To that end, each frame has a fixed integer \textit{priority}.  The active frame with the highest priority determines the system inputs.

If no frame is active at a time $t$, then depending on the configuration of the test case, default values can be used for each input channel, or an error can be thrown (ending the test).

\subsubsection{Relatively-evaluated outputs}
Now, we can re-examine our Evaluation Point data structure.  The point's time interval is a relative time interval, where $t=0$ is the time at which the point's condition is met.  This interval can only be translated to an absolute time interval at run-time, not the time the test case is defined.

\newpage
\section{Implementation}
We have defined what a test case is a fundamental level (namely, a verification of an input-to-output mapping).  We have also described MicroGrader's test case structure, which can represent a reasonable subset of all conceivable test cases.  So far, we understand \textit{what} MicroGrader does.  This section details \textit{how} MicroGrader does it.  The implementation imposes some additional restrictions on the kinds of systems MicroGrader can evaluate.  Most notably, the method in which MicroGrader ``injects" pre-defined inputs to the embedded system makes it unsuitable for grading some types of projects.

\subsection{Overall architecture}
\label{sec:architecture}
MicroGrader uses a client-server model.  The server is not a web server, but rather a Python program running locally on a student's laptop or desktop machine.  That program connects to the client -- namely the embedded system being evaluated -- via USB serial.  The client is a thin wrapper of certain aspects of the embedded system's I/O functionality, which allows the server to ``drive" system inputs and observe results.

This key design choice stemmed from a desire to change the execution of an embedded program as little as possible.  A lightweight approach on the embedded side reduces the chances of incorrect evaluations (both false positives and false negatives).

\subsection{MicroGrader client}
The embedded client has two modes of operation.  In \textit{test} mode, we need to inject pre-defined inputs into the system instead of taking real input readings.  The software, in lieu of taking a real reading, makes a request via USB serial to the server, which responds with the proper pre-defined input value.  In the current reference implementation of the client, requests are blocking -- the embedded system will simply wait for a response if it expects one.  Currently, MicroGrader only supports one request ``in-flight" at a time.

For system outputs, the client really does produce the output (e.g. a digital write, or displaying something on a screen).  In addition to producing that output, the client reports the output to the server, for later evaluation.  The client also reports certain other system events, such as the beginning of program execution and various Wifi activity.

The other mode of operation on the client-side is \textit{inactive} mode.  In this mode, the client makes no request to the server whatsoever.  That is, the program ought to operate completely normally.  Inputs are read from the actual sensors, and outputs are not reported.  This mode allows students to experiment with an in-progress assignment without worrying about the grader.

The exact data that comprises client requests and reports varies depending on the type of request or report.  The communication protocol is detailed in Appendix A.  One commonality of all requests and reports is an integer timestamp, in milliseconds.  The server will trust this timestamp, and doesn't keep track of its own time during a live test.

Ideally, the functionality of the MicroGrader client should wrapped in a library and should mostly be invisible to the student writing code ``on top" of that library.  It may also be necessary to modify existing libraries that handle I/O functionality.  Details about the way this is accomplished in the reference implementation (for Teensy) can be found in Appendix B.

\subsection{MicroGrader server}
The MicroGrader server does the heavy lifting.  It is responsible for interpreting a test case, and using that test case's information to respond promptly and correctly to requests from the client.  We described the details of how a test case specifies input values in sections \ref{sec:static-inputs} and \ref{sec:frames}.  The server is also responsible for observing the system outputs and grading them according to the test case's specification.  The client has no knowledge of the whole test case, it only knows the server responses to its requests.

In the current implementation of the MicroGrader, all of the code is written in Python.  This language was chosen due to its ease of development and flexibility with data types.  Importantly, functions are first-order objects in Python which makes the implementation of a test case much easier (after all, a test case includes aggregator functions and check functions).  Python's popularity make it more convenient for others (especially time-strapped course professors and TAs) to understand and potentially extend the MicroGrader code base.

\subsubsection{Two-stage testing}
Evaluation of a student's system occurs via a two-stage process.  First, the server and client engage in an interactive session to determine how the student's system responds.  The second stage assesses these responses and determines an overall score.

In the interactive stage, the server loads the relevant test case and waits for the client to connect.  Once the client connects, the server processes each message from the client and responds if necessary.  Typically, only requests for input values require responses, though the client may request an acknowledgement response for all messages.  For each incoming message, the server must update all \textit{conditions} in all of the test case's \textit{input frames}.  This will allow the server to determine which frames are currently \textit{active} and what response ought to be sent.  A test case also includes an \textit{end condition}.  Once the end condition is met, the interactive session ends, the server disconnects from the client, and all activity during the session is recorded.

In the evaluation stage, the MicroGrader server considers all the activity that occurred during the interactive session.  MicroGrader scans through the activity log in order to:

\begin{itemize}
\item Determine the time at which each \textit{evaluation point's} condition was met, if it was met at all.  This allows for a point's relative time interval to be converted to an absolute time interval.
\item Reconstruct all the output signals.  Recall that we assume that each new output holds until the next output on the same channel.
\end{itemize}
Each \textit{evaluation point} is examined and evaluated.  Refer back to section \ref{sec:eval-point} for details.  If an point's \textit{condition} was never met, then that point automatically evaluates to |false|.  The boolean results of each point are then aggregated into an overall score for that output channel.  The scores for each output channel are averaged to generate a score for the test as a whole.

MicroGrader is \textbf{not} designed to gracefully handle errors.  If something goes wrong with the communication between the server and the client, or the client sends an invalid request, the session ends immediately and the test fails with an error message.  Similarly, if results of the session cannot be evaluated in the evaluation stage, the test automatically fails with an error message.  These situations are typically indicative of a malformed test case or a software bug that is not the student's responsibility.  Course staff should immediately address these types of issues.


\subsubsection{Reporting results}
For grading purposes, we only care about the numeric final score.  A student, however, would be well served by examining a rich and readable description of the test results.  In the event that a student does not receive full credit, this description should make it clear which \textit{evaluation points} were |false| and why.

Specifically, for each \textit{evaluation point}, the student can see:

\begin{itemize}
\item Whether that point passed, failed, or was not evaluated (which would occur only if a point's condition is not met).
\item A text description of the point's time interval and the \textit{condition} with respect to which that relative interval is defined.
\item The output channel being examined.
\item The expected value on that channel during the specified interval.
\item A description of the check function being used to determine correctness.
\item The portion of the interval for which the observed values were correct.
\item A summary of the observed values on that channel in the specified interval.  This summary is comprised of:
\begin{itemize}
\item The unique values observed during the interval.
\item Whether or not each of those values was correct (as determined by the check function).
\item The portion of the interval for which each of those values was observed.
\end{itemize}
\end{itemize}

Some values do not lend themselves to a textual description.  For example, screen images (a very common type of output), don't readily convert to text.  To handle this, MicroGrader saves all screen images at PNG files in a special directory, and the text description of those images is just the file path.


TODO: describe how results are made less verbose (i.e. PASSED results are shortened)

How does this interact with centralized courseware (e.g. EdX) [Hint: UT Austin's method is good]

\subsubsection{Screen analysis}
[Describe nearness metric with shifts]

[Describe text extraction and fonts]

\subsubsection{Configurability}
Describe what is configurable

\subsubsection{Defaults}
Describe how defaults make life easier

\newpage
\section{Limitations}
[Intro sentence]

\subsection{Latency}
Write about latency (which limits frequency and inhibits tapping into I2C, SPI, etc)

\subsection{Point-by-point evaluation}
Explain how we cant do things like cross correlation

\subsection{Step-wise continuity restriction}
Explain how something like "check that integral of output is X" is incompatible with current software...but not ruled out by the overall approach.


\newpage
\section{Automatic Test Generation}
Explain what this is and why it is helpful

Explain how we generate a recording (i.e. inputs are reported instead of requested)

Explain how recording becomes a test case...this will be a hefty section

\newpage
\section{Results}
TODO: write this once I try out a lot of student 6.S08 code on MicroGrader

\newpage
\section{Future Work}
TODO: write this at the end...when I actually know what I haven't done yet

``Derived output": where the output we care about is some time-dependent function of something the system produces (e.g. the system produces a motor voltage and we care about the position of the wheel driven by the motor)

\newpage
\section{Appendix}

\subsection{Appendix A: Client-server protocol}
\label{sec:protocol}

\subsection{Appendix B: Reference client implementation for Teensy}
\label{sec:teensy}
Describe

\newpage
\section{References}

\end{document}